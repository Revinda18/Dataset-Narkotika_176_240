{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS 1 Temukan minimal 5 penelitian dalam rentang waktu 2020 – 2025 (JURNAL PAPPER)"
      ],
      "metadata": {
        "id": "-x_skWZ23Vh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS 2 MENGUMPULKAN DATA 50 PUTUSAN DI PENGADILAN NARKOTIKA DAN PSIKOTROPIKA TANGERANG"
      ],
      "metadata": {
        "id": "_QnQn2jP2rZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4JOgom-AhYX",
        "outputId": "7b08e26a-4027-4d05-8c59-c2711633a581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im_bI3yE82bM"
      },
      "outputs": [],
      "source": [
        "!pip install pandas requests beautifulsoup4 pdfminer.six lxml > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_BNYxJ884bG"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import urllib\n",
        "from concurrent.futures import ThreadPoolExecutor, wait\n",
        "from datetime import date\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pdfminer.high_level import extract_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgXUDVnmGmpl"
      },
      "source": [
        "##TAHUN 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8be-RHQl87Dn"
      },
      "outputs": [],
      "source": [
        "def create_path(folder_name):\n",
        "    path = os.path.join(os.getcwd(), folder_name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "\n",
        "def open_page(link):\n",
        "    count = 0\n",
        "    while count < 3:\n",
        "        try:\n",
        "            return BeautifulSoup(requests.get(link).text, \"lxml\")\n",
        "        except:\n",
        "            count += 1\n",
        "            time.sleep(5)\n",
        "\n",
        "\n",
        "def get_detail(soup, keyword):\n",
        "    try:\n",
        "        text = (\n",
        "            soup.find(lambda tag: tag.name == \"td\" and keyword in tag.text)\n",
        "            .find_next()\n",
        "            .get_text()\n",
        "            .strip()\n",
        "        )\n",
        "        return text\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def get_pdf(url, path_pdf):\n",
        "    try:\n",
        "        file = urllib.request.urlopen(url)\n",
        "        file_name = os.path.basename(url)\n",
        "        file_content = file.read()\n",
        "        with open(f\"{path_pdf}/{file_name}\", \"wb\") as out_file:\n",
        "            out_file.write(file_content)\n",
        "        return io.BytesIO(file_content), file_name\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\", \"\")\n",
        "    text = text.replace(\"Disclaimer\\n\", \"\")\n",
        "    text = text.replace(\n",
        "        \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_data(link, keyword_url, path_output, path_pdf, today):\n",
        "    soup = open_page(link)\n",
        "    table = soup.find(\"table\", {\"class\": \"table\"})\n",
        "    judul = table.find(\"h2\").text if table.find(\"h2\") else \"\"\n",
        "\n",
        "    nomor = get_detail(table, \"Nomor\")\n",
        "    tingkat_proses = get_detail(table, \"Tingkat Proses\")\n",
        "    klasifikasi = get_detail(table, \"Klasifikasi\")\n",
        "    kata_kunci = get_detail(table, \"Kata Kunci\")\n",
        "    tahun = get_detail(table, \"Tahun\")\n",
        "    tanggal_register = get_detail(table, \"Tanggal Register\")\n",
        "    lembaga_peradilan = get_detail(table, \"Lembaga Peradilan\")\n",
        "    jenis_lembaga_peradilan = get_detail(table, \"Jenis Lembaga Peradilan\")\n",
        "    hakim_ketua = get_detail(table, \"Hakim Ketua\")\n",
        "    hakim_anggota = get_detail(table, \"Hakim Anggota\")\n",
        "    panitera = get_detail(table, \"Panitera\")\n",
        "    amar = get_detail(table, \"Amar\")\n",
        "    amar_lainnya = get_detail(table, \"Amar Lainnya\")\n",
        "    catatan_amar = get_detail(table, \"Catatan Amar\")\n",
        "    tanggal_musyawarah = get_detail(table, \"Tanggal Musyawarah\")\n",
        "    tanggal_dibacakan = get_detail(table, \"Tanggal Dibacakan\")\n",
        "    kaidah = get_detail(table, \"Kaidah\")\n",
        "    status = get_detail(table, \"Status\")\n",
        "    abstrak = get_detail(table, \"Abstrak\")\n",
        "\n",
        "       # === Tambahan filter agar tidak mengambil putusan dengan status BERKEKUATAN HUKUM TETAP ===\n",
        "    if \"berkekuatan hukum tetap\" in status.lower():\n",
        "        print(f\"❌ Dilewati karena sudah inkracht: {judul}\")\n",
        "        return  # langsung hentikan fungsi ini\n",
        "\n",
        "\n",
        "    try:\n",
        "        link_pdf = soup.find(\"a\", href=re.compile(r\"/pdf/\"))[\"href\"]\n",
        "        file_pdf, file_name_pdf = get_pdf(link_pdf, path_pdf)\n",
        "        text_pdf = extract_text(file_pdf)\n",
        "        text_pdf = clean_text(text_pdf)\n",
        "    except:\n",
        "        link_pdf = \"\"\n",
        "        text_pdf = \"\"\n",
        "        file_name_pdf = \"\"\n",
        "\n",
        "    data = [\n",
        "        judul,\n",
        "        nomor,\n",
        "        tingkat_proses,\n",
        "        klasifikasi,\n",
        "        kata_kunci,\n",
        "        tahun,\n",
        "        tanggal_register,\n",
        "        lembaga_peradilan,\n",
        "        jenis_lembaga_peradilan,\n",
        "        hakim_ketua,\n",
        "        hakim_anggota,\n",
        "        panitera,\n",
        "        amar,\n",
        "        amar_lainnya,\n",
        "        catatan_amar,\n",
        "        tanggal_musyawarah,\n",
        "        tanggal_dibacakan,\n",
        "        kaidah,\n",
        "        status,\n",
        "        abstrak,\n",
        "        link,\n",
        "        link_pdf,\n",
        "        file_name_pdf,\n",
        "        text_pdf,\n",
        "    ]\n",
        "    result = pd.DataFrame(\n",
        "        [data],\n",
        "        columns=[\n",
        "            \"judul\",\n",
        "            \"nomor\",\n",
        "            \"tingkat_proses\",\n",
        "            \"klasifikasi\",\n",
        "            \"kata_kunci\",\n",
        "            \"tahun\",\n",
        "            \"tanggal_register\",\n",
        "            \"lembaga_peradilan\",\n",
        "            \"jenis_lembaga_peradilan\",\n",
        "            \"hakim_ketua\",\n",
        "            \"hakim_anggota\",\n",
        "            \"panitera\",\n",
        "            \"amar\",\n",
        "            \"amar_lainnya\",\n",
        "            \"catatan_amar\",\n",
        "            \"tanggal_musyawarah\",\n",
        "            \"tanggal_dibacakan\",\n",
        "            \"kaidah\",\n",
        "            \"status\",\n",
        "            \"abstrak\",\n",
        "            \"link\",\n",
        "            \"link_pdf\",\n",
        "            \"file_name_pdf\",\n",
        "            \"text_pdf\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    keyword_url = keyword_url.replace(\"/\", \" \")\n",
        "    if keyword_url.startswith(\"https\"):\n",
        "        keyword_url = \"\"\n",
        "    destination = f\"{path_output}/putusan_ma_{keyword_url}_{today}\"\n",
        "    if not os.path.isfile(f\"{destination}.csv\"):\n",
        "        result.to_csv(f\"{destination}.csv\", header=True, index=False)\n",
        "    else:\n",
        "        result.to_csv(f\"{destination}.csv\", mode=\"a\", header=False, index=False)\n",
        "\n",
        "\n",
        "def run_process(keyword_url, page, sort_date, path_output, path_pdf, today):\n",
        "    if keyword_url.startswith(\"https\"):\n",
        "        link = f\"{keyword_url}&page={page}\"\n",
        "    else:\n",
        "        link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword_url}&page={page}\"\n",
        "    if sort_date:\n",
        "        link = f\"{link}&obf=TANGGAL_PUTUS&obm=desc\"\n",
        "\n",
        "    soup = open_page(link)\n",
        "    links = soup.find_all(\"a\", {\"href\": re.compile(\"/direktori/putusan\")})\n",
        "\n",
        "    for link in links:\n",
        "        extract_data(link[\"href\"], keyword_url, path_output, path_pdf, today)\n",
        "\n",
        "\n",
        "def run_scraper(keyword=None, url=None, sort_date=True, download_pdf=True):\n",
        "    if not keyword and not url:\n",
        "        print(\"Please provide a keyword or URL\")\n",
        "        return\n",
        "\n",
        "    path_output = '/content/drive/MyDrive/UTS TKI/CSV'\n",
        "    path_pdf = '/content/drive/MyDrive/UTS TKI/PDF'\n",
        "    today = date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword}&page=1\"\n",
        "    if url:\n",
        "        link = url\n",
        "\n",
        "    soup = open_page(link)\n",
        "    last_page = int(soup.find_all(\"a\", {\"class\": \"page-link\"})[-1].get(\"data-ci-pagination-page\"))\n",
        "\n",
        "    if url:\n",
        "        print(f\"Scraping with url: {url} - {20 * last_page} data - {last_page} page\")\n",
        "    else:\n",
        "        print(f\"Scraping with keyword: {keyword} - {20 * last_page} data - {last_page} page\")\n",
        "\n",
        "    if url:\n",
        "        keyword_url = url\n",
        "    else:\n",
        "        keyword_url = keyword\n",
        "\n",
        "    futures = []\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        for page in range(last_page):\n",
        "            futures.append(\n",
        "                executor.submit(run_process, keyword_url, page + 1, sort_date, path_output, path_pdf, today)\n",
        "            )\n",
        "    wait(futures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbqNvhvC9IBb",
        "outputId": "a6c44a04-5728-4b38-feba-e82e5a319e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping with url: https://putusan3.mahkamahagung.go.id/search.html?q=&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=&court=097598PN66|097598PN66%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&t_put=2024&t_reg=&t_upl=&t_pr= - 300 data - 15 page\n"
          ]
        }
      ],
      "source": [
        "# Download Putusan di Pengadilan NARKOTIKA DAN PSIKOTROPIKA TANGERANG\n",
        "run_scraper(url=\"https://putusan3.mahkamahagung.go.id/search.html?q=&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=&court=097598PN66|097598PN66%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&t_put=2024&t_reg=&t_upl=&t_pr=\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCWpBXFlGyGx"
      },
      "source": [
        "##TAHUN 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZEDMw_6GyXl"
      },
      "outputs": [],
      "source": [
        "def create_path(folder_name):\n",
        "    path = os.path.join(os.getcwd(), folder_name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "\n",
        "def open_page(link):\n",
        "    count = 0\n",
        "    while count < 3:\n",
        "        try:\n",
        "            return BeautifulSoup(requests.get(link).text, \"lxml\")\n",
        "        except:\n",
        "            count += 1\n",
        "            time.sleep(5)\n",
        "\n",
        "\n",
        "def get_detail(soup, keyword):\n",
        "    try:\n",
        "        text = (\n",
        "            soup.find(lambda tag: tag.name == \"td\" and keyword in tag.text)\n",
        "            .find_next()\n",
        "            .get_text()\n",
        "            .strip()\n",
        "        )\n",
        "        return text\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def get_pdf(url, path_pdf):\n",
        "    try:\n",
        "        file = urllib.request.urlopen(url)\n",
        "        file_name = os.path.basename(url)\n",
        "        file_content = file.read()\n",
        "        with open(f\"{path_pdf}/{file_name}\", \"wb\") as out_file:\n",
        "            out_file.write(file_content)\n",
        "        return io.BytesIO(file_content), file_name\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\", \"\")\n",
        "    text = text.replace(\"Disclaimer\\n\", \"\")\n",
        "    text = text.replace(\n",
        "        \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_data(link, keyword_url, path_output, path_pdf, today):\n",
        "    soup = open_page(link)\n",
        "    table = soup.find(\"table\", {\"class\": \"table\"})\n",
        "    judul = table.find(\"h2\").text if table.find(\"h2\") else \"\"\n",
        "\n",
        "    nomor = get_detail(table, \"Nomor\")\n",
        "    tingkat_proses = get_detail(table, \"Tingkat Proses\")\n",
        "    klasifikasi = get_detail(table, \"Klasifikasi\")\n",
        "    kata_kunci = get_detail(table, \"Kata Kunci\")\n",
        "    tahun = get_detail(table, \"Tahun\")\n",
        "    tanggal_register = get_detail(table, \"Tanggal Register\")\n",
        "    lembaga_peradilan = get_detail(table, \"Lembaga Peradilan\")\n",
        "    jenis_lembaga_peradilan = get_detail(table, \"Jenis Lembaga Peradilan\")\n",
        "    hakim_ketua = get_detail(table, \"Hakim Ketua\")\n",
        "    hakim_anggota = get_detail(table, \"Hakim Anggota\")\n",
        "    panitera = get_detail(table, \"Panitera\")\n",
        "    amar = get_detail(table, \"Amar\")\n",
        "    amar_lainnya = get_detail(table, \"Amar Lainnya\")\n",
        "    catatan_amar = get_detail(table, \"Catatan Amar\")\n",
        "    tanggal_musyawarah = get_detail(table, \"Tanggal Musyawarah\")\n",
        "    tanggal_dibacakan = get_detail(table, \"Tanggal Dibacakan\")\n",
        "    kaidah = get_detail(table, \"Kaidah\")\n",
        "    status = get_detail(table, \"Status\")\n",
        "    abstrak = get_detail(table, \"Abstrak\")\n",
        "\n",
        "       # === Tambahan filter agar tidak mengambil putusan dengan status BERKEKUATAN HUKUM TETAP ===\n",
        "    if \"berkekuatan hukum tetap\" in status.lower():\n",
        "        print(f\"❌ Dilewati karena sudah inkracht: {judul}\")\n",
        "        return  # langsung hentikan fungsi ini\n",
        "\n",
        "\n",
        "    try:\n",
        "        link_pdf = soup.find(\"a\", href=re.compile(r\"/pdf/\"))[\"href\"]\n",
        "        file_pdf, file_name_pdf = get_pdf(link_pdf, path_pdf)\n",
        "        text_pdf = extract_text(file_pdf)\n",
        "        text_pdf = clean_text(text_pdf)\n",
        "    except:\n",
        "        link_pdf = \"\"\n",
        "        text_pdf = \"\"\n",
        "        file_name_pdf = \"\"\n",
        "\n",
        "    data = [\n",
        "        judul,\n",
        "        nomor,\n",
        "        tingkat_proses,\n",
        "        klasifikasi,\n",
        "        kata_kunci,\n",
        "        tahun,\n",
        "        tanggal_register,\n",
        "        lembaga_peradilan,\n",
        "        jenis_lembaga_peradilan,\n",
        "        hakim_ketua,\n",
        "        hakim_anggota,\n",
        "        panitera,\n",
        "        amar,\n",
        "        amar_lainnya,\n",
        "        catatan_amar,\n",
        "        tanggal_musyawarah,\n",
        "        tanggal_dibacakan,\n",
        "        kaidah,\n",
        "        status,\n",
        "        abstrak,\n",
        "        link,\n",
        "        link_pdf,\n",
        "        file_name_pdf,\n",
        "        text_pdf,\n",
        "    ]\n",
        "    result = pd.DataFrame(\n",
        "        [data],\n",
        "        columns=[\n",
        "            \"judul\",\n",
        "            \"nomor\",\n",
        "            \"tingkat_proses\",\n",
        "            \"klasifikasi\",\n",
        "            \"kata_kunci\",\n",
        "            \"tahun\",\n",
        "            \"tanggal_register\",\n",
        "            \"lembaga_peradilan\",\n",
        "            \"jenis_lembaga_peradilan\",\n",
        "            \"hakim_ketua\",\n",
        "            \"hakim_anggota\",\n",
        "            \"panitera\",\n",
        "            \"amar\",\n",
        "            \"amar_lainnya\",\n",
        "            \"catatan_amar\",\n",
        "            \"tanggal_musyawarah\",\n",
        "            \"tanggal_dibacakan\",\n",
        "            \"kaidah\",\n",
        "            \"status\",\n",
        "            \"abstrak\",\n",
        "            \"link\",\n",
        "            \"link_pdf\",\n",
        "            \"file_name_pdf\",\n",
        "            \"text_pdf\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    keyword_url = keyword_url.replace(\"/\", \" \")\n",
        "    if keyword_url.startswith(\"https\"):\n",
        "        keyword_url = \"\"\n",
        "    destination = f\"{path_output}/putusan_ma_{keyword_url}_{today}\"\n",
        "    if not os.path.isfile(f\"{destination}.csv\"):\n",
        "        result.to_csv(f\"{destination}.csv\", header=True, index=False)\n",
        "    else:\n",
        "        result.to_csv(f\"{destination}.csv\", mode=\"a\", header=False, index=False)\n",
        "\n",
        "\n",
        "def run_process(keyword_url, page, sort_date, path_output, path_pdf, today):\n",
        "    if keyword_url.startswith(\"https\"):\n",
        "        link = f\"{keyword_url}&page={page}\"\n",
        "    else:\n",
        "        link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword_url}&page={page}\"\n",
        "    if sort_date:\n",
        "        link = f\"{link}&obf=TANGGAL_PUTUS&obm=desc\"\n",
        "\n",
        "    soup = open_page(link)\n",
        "    links = soup.find_all(\"a\", {\"href\": re.compile(\"/direktori/putusan\")})\n",
        "\n",
        "    for link in links:\n",
        "        extract_data(link[\"href\"], keyword_url, path_output, path_pdf, today)\n",
        "\n",
        "\n",
        "def run_scraper(keyword=None, url=None, sort_date=True, download_pdf=True):\n",
        "    if not keyword and not url:\n",
        "        print(\"Please provide a keyword or URL\")\n",
        "        return\n",
        "\n",
        "    path_output = '/content/drive/MyDrive/UTS TKI/CSV 2025'\n",
        "    path_pdf = '/content/drive/MyDrive/UTS TKI/PDF 2025'\n",
        "    today = date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword}&page=1\"\n",
        "    if url:\n",
        "        link = url\n",
        "\n",
        "    soup = open_page(link)\n",
        "    last_page = int(soup.find_all(\"a\", {\"class\": \"page-link\"})[-1].get(\"data-ci-pagination-page\"))\n",
        "\n",
        "    if url:\n",
        "        print(f\"Scraping with url: {url} - {20 * last_page} data - {last_page} page\")\n",
        "    else:\n",
        "        print(f\"Scraping with keyword: {keyword} - {20 * last_page} data - {last_page} page\")\n",
        "\n",
        "    if url:\n",
        "        keyword_url = url\n",
        "    else:\n",
        "        keyword_url = keyword\n",
        "\n",
        "    futures = []\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        for page in range(last_page):\n",
        "            futures.append(\n",
        "                executor.submit(run_process, keyword_url, page + 1, sort_date, path_output, path_pdf, today)\n",
        "            )\n",
        "    wait(futures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtfSt5ZVGyeo",
        "outputId": "1afa5445-7005-4961-ff88-51e8f56b27e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping with url: https://putusan3.mahkamahagung.go.id/search.html?q=&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=&court=097598PN66|097598PN66%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&t_put=2025&t_reg=&t_upl=&t_pr= - 40 data - 2 page\n"
          ]
        }
      ],
      "source": [
        "# Download Putusan di Pengadilan NARKOTIKA DAN PSIKOTROPIKA TANGERANG\n",
        "run_scraper(url=\"https://putusan3.mahkamahagung.go.id/search.html?q=&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=&court=097598PN66|097598PN66%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&t_put=2025&t_reg=&t_upl=&t_pr=\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGHv0QB3lwds"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview.csv"
      ],
      "metadata": {
        "id": "hnh7T1Y93LKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install PyPDF2\n",
        "\n",
        "import pdfplumber\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# =======================\n",
        "# KONFIGURASI FOLDER\n",
        "# =======================\n",
        "input_folder = '/content/drive/MyDrive/TUGAS 3 UTS TKI'  # Folder berisi 50 PDF putusan\n",
        "output_csv = '/content/drive/MyDrive/TUGAS 2 UTS TKI/Overview.csv'\n",
        "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
        "\n",
        "# =======================\n",
        "# FUNGSI EKSTRAKSI TEKS\n",
        "# =======================\n",
        "def read_pdf_text(pdf_path):\n",
        "    \"\"\"Baca seluruh teks dari file PDF\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Gagal membaca PDF {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "def extract_metadata(text):\n",
        "    \"\"\"Ekstraksi metadata penting dari isi putusan\"\"\"\n",
        "    # No Putusan\n",
        "    match_no = re.search(r'(nomor|no)[\\s:.]*([^\\s\\n;,]*)', text, re.IGNORECASE)\n",
        "    no_putusan = match_no.group(2).strip() if match_no else ''\n",
        "\n",
        "    # Lembaga Peradilan\n",
        "    match_lembaga = re.search(r'pengadilan\\s+negeri\\s+[a-z\\s]+', text, re.IGNORECASE)\n",
        "    lembaga = match_lembaga.group(0).title().strip() if match_lembaga else 'PN Tidak Diketahui'\n",
        "\n",
        "    # Barang Bukti\n",
        "    match_bb = re.search(r'barang bukti(.*?)(menimbang|mengadili)', text, re.DOTALL | re.IGNORECASE)\n",
        "    barang_bukti = match_bb.group(1).strip() if match_bb else ''\n",
        "\n",
        "    return no_putusan, lembaga, barang_bukti\n",
        "\n",
        "def extract_amar_putusan(text):\n",
        "    \"\"\"Ekstraksi bagian amar putusan\"\"\"\n",
        "    match_amar = re.search(r'm\\s*e\\s*n\\s*g\\s*a\\s*d\\s*i\\s*l\\s*i\\s*:?([\\s\\S]*)', text, re.IGNORECASE)\n",
        "    if match_amar:\n",
        "        amar_text = match_amar.group(1)\n",
        "        closing_phrases = ['demikian diputuskan', 'ditetapkan di', 'panitera pengganti', 'hakim ketua']\n",
        "        for phrase in closing_phrases:\n",
        "            amar_text = re.split(phrase, amar_text, flags=re.IGNORECASE)[0]\n",
        "        return amar_text.strip()\n",
        "    return ''\n",
        "\n",
        "# =======================\n",
        "# PROSES EKSTRAKSI\n",
        "# =======================\n",
        "rows = []\n",
        "print(\"🚀 Memulai ekstraksi data dari PDF...\")\n",
        "\n",
        "file_list = sorted(os.listdir(input_folder))\n",
        "if not file_list:\n",
        "    print(f\"⚠️ Folder input '{input_folder}' kosong.\")\n",
        "\n",
        "for i, filename in enumerate(file_list):\n",
        "    if filename.lower().endswith('.pdf'):\n",
        "        pdf_path = os.path.join(input_folder, filename)\n",
        "        text = read_pdf_text(pdf_path)\n",
        "\n",
        "        no_putusan, lembaga, barang_bukti = extract_metadata(text)\n",
        "        amar_putusan = extract_amar_putusan(text)\n",
        "\n",
        "        rows.append({\n",
        "            'No': i + 1,\n",
        "            'No Putusan': no_putusan,\n",
        "            'Lembaga Peradilan': lembaga,\n",
        "            'Barang Bukti': barang_bukti,\n",
        "            'Amar Putusan': amar_putusan\n",
        "        })\n",
        "        print(f\"✅ Berhasil ekstrak: {filename}\")\n",
        "\n",
        "# =======================\n",
        "# SIMPAN KE CSV\n",
        "# =======================\n",
        "if rows:\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\n🎯 Selesai! Data disimpan ke: {output_csv}\")\n",
        "else:\n",
        "    print(\"\\n⚠️ Tidak ada data yang berhasil diproses.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy_akm98vX31",
        "outputId": "7490ef70-7556-45f5-ca65-f6e7811a2ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (0.11.7)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (5.0.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "🚀 Memulai ekstraksi data dari PDF...\n",
            "✅ Berhasil ekstrak: zaeef323355aacbca849313530353033.pdf\n",
            "✅ Berhasil ekstrak: zaeef3233622a06e9869313530353035.pdf\n",
            "✅ Berhasil ekstrak: zaeef323384f470caeed313530353038.pdf\n",
            "✅ Berhasil ekstrak: zaeef3235ad378e890d0313530363036.pdf\n",
            "✅ Berhasil ekstrak: zaef00c44e399488b191323332303539.pdf\n",
            "✅ Berhasil ekstrak: zaef02337c28b8728579313930393232(1).pdf\n",
            "✅ Berhasil ekstrak: zaef02337c28b8728579313930393232.pdf\n",
            "✅ Berhasil ekstrak: zaef02337dac3f0c9fba313930393234(1).pdf\n",
            "✅ Berhasil ekstrak: zaef02337dac3f0c9fba313930393234.pdf\n",
            "✅ Berhasil ekstrak: zaef02f6fd097670bb7e313832383530(1).pdf\n",
            "✅ Berhasil ekstrak: zaef02f6fd097670bb7e313832383530.pdf\n",
            "✅ Berhasil ekstrak: zaef02f724242e1cb803313832393535(1).pdf\n",
            "✅ Berhasil ekstrak: zaef02f724242e1cb803313832393535.pdf\n",
            "✅ Berhasil ekstrak: zaef02f7312df6b09419313833303137.pdf\n",
            "✅ Berhasil ekstrak: zaef03a4dc884abc8f10313531333238(1).pdf\n",
            "✅ Berhasil ekstrak: zaef03a4dc884abc8f10313531333238.pdf\n",
            "✅ Berhasil ekstrak: zaef03a4eee67e72801a313531333538(1).pdf\n",
            "✅ Berhasil ekstrak: zaef03a4eee67e72801a313531333538.pdf\n",
            "✅ Berhasil ekstrak: zaef0818b0097f74b64a303731323339(1).pdf\n",
            "✅ Berhasil ekstrak: zaef0818b0097f74b64a303731323339.pdf\n",
            "✅ Berhasil ekstrak: zaef0818b90e48b6beb9303731323534(1).pdf\n",
            "✅ Berhasil ekstrak: zaef0818b90e48b6beb9303731323534.pdf\n",
            "✅ Berhasil ekstrak: zaef312a78c4263cb705313333333135.pdf\n",
            "✅ Berhasil ekstrak: zaef312a926017d6aa64313333333538.pdf\n",
            "✅ Berhasil ekstrak: zaef312aae14a974b3c7313333343435.pdf\n",
            "✅ Berhasil ekstrak: zaef31e3a5fa3628acad313133383438.pdf\n",
            "✅ Berhasil ekstrak: zaef3cebf7f100668fa8313233363034.pdf\n",
            "✅ Berhasil ekstrak: zaef3d157d2d79f28871313733333137.pdf\n",
            "✅ Berhasil ekstrak: zaef4295d6000b6ebf95313733343337.pdf\n",
            "✅ Berhasil ekstrak: zaef4295da74c41eb912313733343435.pdf\n",
            "✅ Berhasil ekstrak: zaef429625f7d19285a2313733363532.pdf\n",
            "✅ Berhasil ekstrak: zaef4296279287e0c0bd313733363534.pdf\n",
            "✅ Berhasil ekstrak: zaef5495bcb47bb0c076313531393136.pdf\n",
            "✅ Berhasil ekstrak: zaefb6ad0e366880b0f7313131333035.pdf\n",
            "✅ Berhasil ekstrak: zaefb6adfecb784a9798313131323339.pdf\n",
            "✅ Berhasil ekstrak: zaefbb7d90dab7f88a6b313431353434.pdf\n",
            "✅ Berhasil ekstrak: zaefd2fc45b96ef4a1b1313135303431.pdf\n",
            "✅ Berhasil ekstrak: zaefd2fc4abbe58ab34a313135303439.pdf\n",
            "✅ Berhasil ekstrak: zaefd2fc526c45f4a078313135313032.pdf\n",
            "✅ Berhasil ekstrak: zaefd2fc5fee1a04b444313135313235.pdf\n",
            "✅ Berhasil ekstrak: zaf098429d96ab1c969c313330303332.pdf\n",
            "✅ Berhasil ekstrak: zaf09842a6de29b6a720313330303438.pdf\n",
            "✅ Berhasil ekstrak: zaf09842d9c803ba9d0c313330323133.pdf\n",
            "✅ Berhasil ekstrak: zaf09842eb759d168eb2313330323433.pdf\n",
            "✅ Berhasil ekstrak: zaf09842ed753a9abd5b313330323436.pdf\n",
            "✅ Berhasil ekstrak: zaf09843fb3da7ecab31313331303139.pdf\n",
            "✅ Berhasil ekstrak: zaf0984406c7273c8e4e313331303338.pdf\n",
            "✅ Berhasil ekstrak: zaf0984e4a496434827b313432343036.pdf\n",
            "✅ Berhasil ekstrak: zaf0984ea8db3d108eab313432363435.pdf\n",
            "✅ Berhasil ekstrak: zaf09dd2b763049cb502313435343339.pdf\n",
            "✅ Berhasil ekstrak: zaf09dd2b8ac4e30ae8e313435343431.pdf\n",
            "✅ Berhasil ekstrak: zaf09dd2b9f23a02897a313435343433.pdf\n",
            "✅ Berhasil ekstrak: zaf09dd2c4816d8abe04313435353031.pdf\n",
            "✅ Berhasil ekstrak: zaf09dd2d0782a34b7dc313435353231.pdf\n",
            "✅ Berhasil ekstrak: zaf09dd2ef206dc08016313435363132.pdf\n",
            "✅ Berhasil ekstrak: zaf0b4a04a8027128602313532313338.pdf\n",
            "✅ Berhasil ekstrak: zaf0b4a04b503ca4a75c313532313339.pdf\n",
            "✅ Berhasil ekstrak: zaf0b4a04b7792048bd8313532313430.pdf\n",
            "✅ Berhasil ekstrak: zaf0b4a0e1899184984d313532353532.pdf\n",
            "✅ Berhasil ekstrak: zaf0b4a0e41fb8c49f2c313532353536.pdf\n",
            "✅ Berhasil ekstrak: zaf0b4a0eac67fd2b920313532363037.pdf\n",
            "✅ Berhasil ekstrak: zaf0b4af45cc8ed68889313730383533.pdf\n",
            "\n",
            "🎯 Selesai! Data disimpan ke: /content/drive/MyDrive/TUGAS 2 UTS TKI/Overview.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS 3  Preprocessing & Indexing berdasarkan file Overview.csv yang sudah di hasilkan dari Tugas 2."
      ],
      "metadata": {
        "id": "2PabRLWn3vnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install Sastrawi scikit-learn pandas nltk\n",
        "\n",
        "# --- Import library ---\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# --- Load dataset hasil Tugas 2 ---\n",
        "file_path = '/content/drive/MyDrive/TUGAS 2 UTS TKI/Overview.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"✅ Dataset berhasil dimuat!\")\n",
        "print(df.head())\n",
        "\n",
        "#  Tahap 1: Preprocessing Teks\n",
        "\n",
        "\n",
        "# Gabungkan kolom teks penting jadi satu dokumen besar per putusan\n",
        "df['Teks_Gabungan'] = (\n",
        "    df['Barang Bukti'].fillna('') + ' ' +\n",
        "    df['Amar Putusan'].fillna('')\n",
        ")\n",
        "\n",
        "# Fungsi preprocessing: lowercase, hapus tanda baca, stopword removal, stemming\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Hapus karakter non-alfabet\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    # Tokenisasi sederhana\n",
        "    tokens = text.split()\n",
        "    # Hapus stopwords & stemming\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Terapkan preprocessing ke seluruh dokumen\n",
        "df['Preprocessed'] = df['Teks_Gabungan'].apply(preprocess_text)\n",
        "\n",
        "print(\"\\n🧹 Contoh hasil preprocessing:\")\n",
        "print(df[['No', 'Preprocessed']].head())\n",
        "\n",
        "\n",
        "# Tahap 2: Indexing Menggunakan TF-IDF\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(df['Preprocessed'])\n",
        "\n",
        "print(\"\\n📈 TF-IDF indexing selesai!\")\n",
        "print(\"Jumlah dokumen:\", tfidf_matrix.shape[0])\n",
        "print(\"Jumlah fitur (kata unik):\", tfidf_matrix.shape[1])\n",
        "\n",
        "# Tahap 3: Fungsi Pencarian Sederhana\n",
        "\n",
        "\n",
        "def search(query, top_n=5):\n",
        "    \"\"\"Melakukan pencarian teks sederhana dengan cosine similarity\"\"\"\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    # Preprocessing query\n",
        "    query_processed = preprocess_text(query)\n",
        "    query_vec = vectorizer.transform([query_processed])\n",
        "\n",
        "    # Hitung cosine similarity\n",
        "    similarity = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "\n",
        "    # Ambil top-n hasil\n",
        "    results = df.copy()\n",
        "    results['Similarity'] = similarity\n",
        "    results = results.sort_values(by='Similarity', ascending=False).head(top_n)\n",
        "\n",
        "    return results[['No', 'No Putusan', 'Lembaga Peradilan', 'Similarity', 'Barang Bukti', 'Amar Putusan']]\n",
        "\n",
        "# 🧭 Contoh Uji Pencarian\n",
        "\n",
        "query = \"narkotika sabu-sabu\"\n",
        "hasil = search(query)\n",
        "\n",
        "print(f\"\\n🔎 Hasil pencarian untuk query: '{query}'\\n\")\n",
        "print(hasil)\n",
        "\n",
        "#  (Opsional) Simpan hasil preprocessing & index ke file CSV\n",
        "df.to_csv('/content/drive/MyDrive/TUGAS 3 UTS TKI/Preprocessed_Index.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"\\n💾 File hasil preprocessing dan indexing disimpan ke: /content/drive/MyDrive/TUGAS 3 UTS TKI/Preprocessed_Index.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVaCMo-Q4AMq",
        "outputId": "efeedcde-7d24-4103-cd56-1e83e07cf44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset berhasil dimuat!\n",
            "   No            No Putusan  \\\n",
            "0   1  1942/Pid.Sus/2023/PN   \n",
            "1   2  1941/Pid.Sus/2023/PN   \n",
            "2   3  1940/Pid.Sus/2023/PN   \n",
            "3   4   275/Pid.Sus/2024/PN   \n",
            "4   5   264/Pid.Sus/2024/PN   \n",
            "\n",
            "                                   Lembaga Peradilan  \\\n",
            "0  Pengadilan Negeri  Tangerang Yang Mengadili Pe...   \n",
            "1  Pengadilan Negeri  Tangerang Yang Mengadili Pe...   \n",
            "2  Pengadilan Negeri  Tangerang Yang Mengadili Pe...   \n",
            "3  Pengadilan Negeri Tangerang Yang Mengadili Per...   \n",
            "4  Pengadilan Negeri  Tangerang Yang Mengadili Pe...   \n",
            "\n",
            "                                        Barang Bukti  \\\n",
            "0  yang diajukan di persidangan;\\nSetelah  menden...   \n",
            "1  yang diajukan di persidangan;\\nSetelah  menden...   \n",
            "2  yang diajukan di persidangan;\\nSetelah  menden...   \n",
            "3  yang diajukan di persidangan;\\nSetelah  menden...   \n",
            "4  yang diajukan di persidangan;\\nSetelah  menden...   \n",
            "\n",
            "                                        Amar Putusan  \n",
            "0  perkara pidana dengan\\nacara pemeriksaan biasa...  \n",
            "1  perkara pidana dengan\\nacara pemeriksaan biasa...  \n",
            "2  perkara pidana dengan\\nacara pemeriksaan biasa...  \n",
            "3  perkara  pidana dengan\\nacara pemeriksaan bias...  \n",
            "4  perkara  pidana dengan\\nacara pemeriksaan bias...  \n",
            "\n",
            "🧹 Contoh hasil preprocessing:\n",
            "   No                                       Preprocessed\n",
            "0   1  aju sidang dengar baca tuntut pidana aju tuntu...\n",
            "1   2  aju sidang dengar baca tuntut pidana aju tuntu...\n",
            "2   3  aju sidang dengar baca tuntut pidana aju tuntu...\n",
            "3   4  aju sidang dengar baca tuntut pidana aju tuntu...\n",
            "4   5  aju sidang dengar baca tuntut pidana aju tuntu...\n",
            "\n",
            "📈 TF-IDF indexing selesai!\n",
            "Jumlah dokumen: 62\n",
            "Jumlah fitur (kata unik): 4737\n",
            "\n",
            "🔎 Hasil pencarian untuk query: 'narkotika sabu-sabu'\n",
            "\n",
            "    No            No Putusan  \\\n",
            "28  29   707/Pid.Sus/2024/PN   \n",
            "44  45  1250/Pid.Sus/2025/PN   \n",
            "0    1  1942/Pid.Sus/2023/PN   \n",
            "1    2  1941/Pid.Sus/2023/PN   \n",
            "2    3  1940/Pid.Sus/2023/PN   \n",
            "\n",
            "                                    Lembaga Peradilan  Similarity  \\\n",
            "28  Pengadilan Negeri  Tangerang Yang Mengadili Pe...    0.305602   \n",
            "44  Pengadilan Negeri  Tangerang Yang Mengadili Pe...    0.298575   \n",
            "0   Pengadilan Negeri  Tangerang Yang Mengadili Pe...    0.276975   \n",
            "1   Pengadilan Negeri  Tangerang Yang Mengadili Pe...    0.272602   \n",
            "2   Pengadilan Negeri  Tangerang Yang Mengadili Pe...    0.247839   \n",
            "\n",
            "                                         Barang Bukti  \\\n",
            "28  berupa :\\n1)1 (satu) buah tas kosmetik bertuli...   \n",
            "44  yang diajukan di persidangan;\\nSetelah  menden...   \n",
            "0   yang diajukan di persidangan;\\nSetelah  menden...   \n",
            "1   yang diajukan di persidangan;\\nSetelah  menden...   \n",
            "2   yang diajukan di persidangan;\\nSetelah  menden...   \n",
            "\n",
            "                                         Amar Putusan  \n",
            "28  perkara  pidana dengan\\nacara pemeriksaan bias...  \n",
            "44  perkara pidana dengan\\nacara pemeriksaan biasa...  \n",
            "0   perkara pidana dengan\\nacara pemeriksaan biasa...  \n",
            "1   perkara pidana dengan\\nacara pemeriksaan biasa...  \n",
            "2   perkara pidana dengan\\nacara pemeriksaan biasa...  \n",
            "\n",
            "💾 File hasil preprocessing dan indexing disimpan ke: /content/drive/MyDrive/TUGAS 3 UTS TKI/Preprocessed_Index.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}